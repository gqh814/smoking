{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define parameters\n",
    "a_sigma = 0.5\n",
    "a_0 = 1\n",
    "a_c = 0.2\n",
    "a_sb = 0.5\n",
    "a_bsigma = -0.5\n",
    "\n",
    "def u(b, s, sigma, y, p):\n",
    "    return np.exp(a_sigma*sigma)*(a_0 +(y-p*b)**a_c*(1+s*b)**a_sb*(1+sigma*b)**a_bsigma)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = 10\n",
    "u = 2.0           # Baseline enjoyment from smoking\n",
    "theta = 0.02      # Reinforcement effect weight (quadratic in A)\n",
    "phi = 0.95        # Retention rate of addiction\n",
    "beta = 0.1        # Increase in addiction when smoking\n",
    "eta = 0.9         # Base health depreciation per period (if not smoking)\n",
    "gamma = 0.2       # Additional health depreciation from smoking\n",
    "delta = 0.95      # Discount factor\n",
    "cost_smoking = 2.0  # Explicit monetary cost of smoking\n",
    "mu = 0.5\n",
    "\n",
    "# Health utility function: higher H gives more utility.\n",
    "def health_utility(H):\n",
    "    # For example, we might assume utility is concave in health.\n",
    "    return mu * H**(1/2)\n",
    "\n",
    "# Grids for state variables:\n",
    "A_min, A_max, A_steps = 0, 10, 51  # addiction\n",
    "H_min, H_max, H_steps = 0, 10, 51    # health level (higher is better)\n",
    "A_grid = np.linspace(A_min, A_max, A_steps)\n",
    "H_grid = np.linspace(H_min, H_max, H_steps)\n",
    "\n",
    "# Initialize value function and policy\n",
    "V = np.zeros((A_steps, H_steps))\n",
    "policy = np.zeros((A_steps, H_steps), dtype=int)  # 0: no smoking, 1: smoking\n",
    "\n",
    "tol = 1e-4\n",
    "max_iter = 1000\n",
    "\n",
    "def find_nearest_index(x, grid):\n",
    "    return np.abs(grid - x).argmin()\n",
    "A_mesh, H_mesh = np.meshgrid(A_grid, H_grid, indexing='ij')\n",
    "\n",
    "\n",
    "# Value iteration: here, the immediate utility depends on current health.\n",
    "for iteration in range(max_iter):\n",
    "    V_new = np.copy(V)\n",
    "    diff = 0.0\n",
    "    for i, A in enumerate(A_grid):\n",
    "        for j, H in enumerate(H_grid):\n",
    "            value_actions = np.zeros(2)\n",
    "            \n",
    "            # Action 0: Not smoking\n",
    "            # Immediate utility: utility from current health.\n",
    "            immediate_0 = health_utility(H)\n",
    "            # Transition: addiction decays; health depreciates at base rate.\n",
    "            A_next_0 = phi * A\n",
    "            H_next_1 = H * eta  # ensure health doesn't drop below 0\n",
    "            i_next_0 = find_nearest_index(A_next_0, A_grid)\n",
    "            j_next_0 = find_nearest_index(H_next_0, H_grid)\n",
    "            continuation_0 = V[i_next_0, j_next_0]\n",
    "            value_actions[0] = immediate_0 + delta * continuation_0\n",
    "            \n",
    "            # Action 1: Smoking\n",
    "            # Immediate utility: enjoyment from smoking plus current health utility,\n",
    "            # minus explicit monetary cost.\n",
    "            # You could also include a reinforcement term; for instance, theta * A or theta * A^2.\n",
    "            immediate_1 = u + theta * A**2 + health_utility(H) - cost_smoking\n",
    "            # Transition: addiction increases; health deteriorates faster.\n",
    "            A_next_1 = A  \n",
    "            H_next_1 = H * (eta - gamma)\n",
    "            i_next_1 = find_nearest_index(A_next_1, A_grid)\n",
    "            j_next_1 = find_nearest_index(H_next_1, H_grid)\n",
    "            continuation_1 = V[i_next_1, j_next_1]\n",
    "            value_actions[1] = immediate_1 + delta * continuation_1\n",
    "            \n",
    "            best_action = np.argmax(value_actions)\n",
    "            V_new[i, j] = value_actions[best_action]\n",
    "            policy[i, j] = best_action\n",
    "            \n",
    "            diff = max(diff, np.abs(V_new[i, j] - V[i, j]))\n",
    "    V = V_new\n",
    "    if diff < tol:\n",
    "        print(f\"Value iteration converged after {iteration+1} iterations.\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Value iteration did not converge.\")\n",
    "\n",
    "# Plot the optimal policy: 0: No Smoking, 1: Smoking\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.contourf(H_mesh, A_mesh, policy, levels=[-0.5,0.5,1.5], cmap='RdYlGn_r')\n",
    "cbar = plt.colorbar(ticks=[0, 1])\n",
    "cbar.ax.set_yticklabels(['No Smoking','Smoking'])\n",
    "plt.xlabel('Health Level (H)')\n",
    "plt.ylabel('Addictive Stock (A)')\n",
    "plt.title('Optimal Smoking Policy (Higher H = better health)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
